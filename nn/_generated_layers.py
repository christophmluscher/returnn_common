"""
This file is auto-generated by _generate_layers.py.
RETURNN: 1.20211217.094558+git.74cfd06

These are the RETURNN layers directly wrapped.
Note that we intentionally exclude some layers or options for more consistency.
Please file an issue if you miss something.
"""

from __future__ import annotations
from typing import Union, Optional, Tuple, List, Sequence, Dict, Set, Any
from returnn.util.basic import NotSpecified
# noinspection PyProtectedMember
from returnn.tf.util.data import Dim, _ImplicitDim
from .base import NameCtx, _ReturnnWrappedLayerBase, Layer, LayerRef, LayerState, make_layer


class _Base(_ReturnnWrappedLayerBase):
  """
  This is the base class for all layers.
  Every layer by default has a list of source layers `sources` and defines `self.output` which is of type :class:`Data`.
  It shares some common functionality across all layers, such as explicitly defining the output format,
  some parameter regularization, and more.

  If you want to implement your own layer::

      class YourOwnLayer(_ConcatInputLayer):  # e.g. either _ConcatInputLayer or LayerBase as a base
          " some docstring "
          layer_class = "your_layer_name"

          def __init__(self, your_kwarg1, your_opt_kwarg2=None, **kwargs):
              " docstring, document the args! "
              super(YourOwnLayer, self).__init__(**kwargs)
              # Now we need to set self.output, which must be of type :class:`Data`.
              # It is set at this point to whatever we got from `self.get_out_data_from_opts()`,
              # so it is enough if we set self.output.placeholder and self.output.size_placeholder,
              # but we could also reset self.output.
              self.output.placeholder = self.input_data.placeholder + 42  # whatever you want to do
              # If you don't modify the sizes (e.g. sequence-length), just copy the input sizes.
              self.output.size_placeholder = self.input_data.size_placeholder.copy()

          @classmethod
          def get_out_data_from_opts(cls, **kwargs):
              " This is supposed to return a :class:`Data` instance as a template, given the arguments. "
              # example, just the same as the input:
              return get_concat_sources_data_template(kwargs["sources"], name="%s_output" % kwargs["name"])

  """
  returnn_layer_class = None
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               out_dim: Optional[Dim] = NotSpecified,
               out_shape: Optional[Union[Set[Union[Dim, _ImplicitDim]], Tuple, List]] = NotSpecified,
               in_dim: Optional[Dim] = NotSpecified,
               param_device: Optional[str] = NotSpecified,
               only_on_eval: bool = NotSpecified,
               only_on_search: bool = NotSpecified,
               l2: Optional[float] = NotSpecified,
               darc1: Optional[float] = NotSpecified,
               spatial_smoothing: Optional[float] = NotSpecified,
               param_variational_noise: Optional[float] = NotSpecified,
               updater_opts: Optional[Dict[str]] = NotSpecified,
               need_last: bool = NotSpecified,
               trainable: Optional[bool] = NotSpecified,
               custom_param_importer: Optional[Union[str, callable]] = NotSpecified,
               ):
    """
    Usually the arguments, when specified in the network dict,
    are going through :func:`transform_config_dict`, before they are passed to here.
    See :func:`TFNetwork.construct_from_dict`.

    :param Dim|None out_dim: output feature dim tag
    :param set[Dim|_ImplicitDim]|tuple|list|None out_shape:
      verifies the output shape (dim tags). See :func:`Data.verify_out_shape`.
    :param Dim|None in_dim: input feature dim tag
    :param str|None param_device: e.g. "CPU", etc. any valid name for tf.device.
      see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/device_name_utils.h
    :param bool only_on_eval: if True, this layer will only be calculated in eval
    :param bool only_on_search: if True, this layer will only be calculated when search is done
    :param float|None l2: for constraints
    :param float|None darc1: for constraints. see Generalization in Deep Learning, https://arxiv.org/abs/1710.05468
    :param float|None spatial_smoothing: see :func:`returnn.tf.util.basic.spatial_smoothing_energy`
    :param float|None param_variational_noise: adds variational noise to the params during training
    :param dict[str]|None updater_opts: accepts similar opts as TFUpdater, e.g. "optimizer", "learning_rate", ...
    :param bool need_last: Inside :class:`RecLayer`, make sure that we can access the last frame.
      Similar to ``is_output_layer, but this is specifically about the last frame,
      i.e. it does not trigger accumulation.
    :param bool|None trainable: whether the parameters of this layer will be trained.
      default (None) inherits from the parent layer if there is one, or otherwise True.
    :param str|callable|None custom_param_importer: used by :func:`set_param_values_by_dict`
    """
    super().__init__()
    self.out_dim = out_dim
    self.out_shape = out_shape
    self.in_dim = in_dim
    self.param_device = param_device
    self.only_on_eval = only_on_eval
    self.only_on_search = only_on_search
    self.l2 = l2
    self.darc1 = darc1
    self.spatial_smoothing = spatial_smoothing
    self.param_variational_noise = param_variational_noise
    self.updater_opts = updater_opts
    self.need_last = need_last
    self.trainable = trainable
    self.custom_param_importer = custom_param_importer

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'out_dim': self.out_dim,
      'out_shape': self.out_shape,
      'in_dim': self.in_dim,
      'param_device': self.param_device,
      'only_on_eval': self.only_on_eval,
      'only_on_search': self.only_on_search,
      'L2': self.l2,
      'darc1': self.darc1,
      'spatial_smoothing': self.spatial_smoothing,
      'param_variational_noise': self.param_variational_noise,
      'updater_opts': self.updater_opts,
      'need_last': self.need_last,
      'trainable': self.trainable,
      'custom_param_importer': self.custom_param_importer,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return opts

  __call__ = _ReturnnWrappedLayerBase.__call__  # abstract


# noinspection PyShadowingBuiltins,PyShadowingNames
def copy(
         source: LayerRef,
         *,
         in_dim: Optional[Dim] = NotSpecified,
         out_dim: Optional[Dim] = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This layer does nothing, it copies its input.
  If multiple sources are provided, they are concatenated in the feature-dim.

  :param LayerRef source:
  :param Dim|None in_dim:
  :param Dim|None out_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'in_dim': in_dim,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'copy',
    'from': source,
    **args}, name=name or 'copy')


# noinspection PyShadowingBuiltins,PyShadowingNames
def scaled_gradient(
                    source: LayerRef,
                    *,
                    scale: float,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Just tf.identity in the forward pass.
  Scales the gradient by some factor in backprop.
  Can be used as gradient reversal layer (with negative factor).
  Uses :func:`returnn.tf.util.basic.scaled_gradient`, or :func:`tf.stop_gradient`

  :param LayerRef source:
  :param float scale: if 0., will use tf.stop_gradient
  :param str|NameCtx|None name:
  """
  args = {
    'scale': scale,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'scaled_grad',
    'from': source,
    **args}, name=name or 'scaled_gradient')


class BatchNorm(_Base):
  """
  Implements batch-normalization (https://arxiv.org/abs/1502.03167) as a separate layer.

  Also see :class:`NormLayer`.
  """
  returnn_layer_class = 'batch_norm'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               use_shift: bool = NotSpecified,
               use_std: bool = NotSpecified,
               use_sample: float = NotSpecified,
               force_sample: bool = NotSpecified,
               momentum: float = NotSpecified,
               epsilon: float = NotSpecified,
               update_sample_only_in_training: bool = NotSpecified,
               delay_sample_update: bool = NotSpecified,
               param_version: int = NotSpecified,
               gamma_init: Union[str, float] = NotSpecified,
               beta_init: Union[str, float] = NotSpecified,
               masked_time: bool = NotSpecified,
               **kwargs):
    """
    The default settings for these variables are set in the function "batch_norm" of the LayerBase. If you do not want
    to change them you can leave them undefined here.
    With our default settings:

    - In training: use_sample=0, i.e. not using running average, using current batch mean/var.
    - Not in training (e.g. eval): use_sample=1, i.e. using running average, not using current batch mean/var.
    - The running average includes the statistics of the current batch.
    - The running average is also updated when not training.

    :param bool use_shift:
    :param bool use_std:
    :param float use_sample: defaults to 0.0 which is used in training
    :param bool force_sample: even in eval, use the use_sample factor
    :param float momentum: for the running average of sample_mean and sample_std
    :param float epsilon:
    :param bool update_sample_only_in_training:
    :param bool delay_sample_update:
    :param int param_version: 0 or 1
    :param str|float gamma_init: see :func:`returnn.tf.util.basic.get_initializer`, for the scale
    :param str|float beta_init: see :func:`returnn.tf.util.basic.get_initializer`, for the mean
    :param bool masked_time: flatten and mask input tensor
    """
    super().__init__(**kwargs)
    self.use_shift = use_shift
    self.use_std = use_std
    self.use_sample = use_sample
    self.force_sample = force_sample
    self.momentum = momentum
    self.epsilon = epsilon
    self.update_sample_only_in_training = update_sample_only_in_training
    self.delay_sample_update = delay_sample_update
    self.param_version = param_version
    self.gamma_init = gamma_init
    self.beta_init = beta_init
    self.masked_time = masked_time

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'use_shift': self.use_shift,
      'use_std': self.use_std,
      'use_sample': self.use_sample,
      'force_sample': self.force_sample,
      'momentum': self.momentum,
      'epsilon': self.epsilon,
      'update_sample_only_in_training': self.update_sample_only_in_training,
      'delay_sample_update': self.delay_sample_update,
      'param_version': self.param_version,
      'gamma_init': self.gamma_init,
      'beta_init': self.beta_init,
      'masked_time': self.masked_time,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef,
               ) -> Layer:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    return make_layer({
      'class': 'batch_norm',
      'from': source,
      **self.get_opts()}, module=self)


# noinspection PyShadowingBuiltins,PyShadowingNames
def layer_norm(
               source: LayerRef,
               *,
               in_dim: Optional[Dim] = NotSpecified,
               out_dim: Optional[Dim] = NotSpecified,
               epsilon: float = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Applies `layer-normalization <https://arxiv.org/abs/1607.06450>`__.

  Note that we *just* normalize over the feature-dim axis here.
  This is consistent to the default behavior of :class:`tf.keras.layers.LayerNormalization`
  and also how it is commonly used in many models, including Transformer.

  However, there are cases where it would be common to normalize over all axes except batch-dim,
  or all axes except batch and time.
  For a more generic variant, see :class:`NormLayer`.

  :param LayerRef source:
  :param Dim|None in_dim:
  :param Dim|None out_dim:
  :param float epsilon:
  :param str|NameCtx|None name:
  """
  args = {
    'in_dim': in_dim,
    'out_dim': out_dim,
    'epsilon': epsilon,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'layer_norm',
    'from': source,
    **args}, name=name or 'layer_norm')


# noinspection PyShadowingBuiltins,PyShadowingNames
def normalize(
              source: LayerRef,
              *,
              axis: Union[Dim, List[Dim]],
              param_shape: Union[Dim, List[Dim], Tuple[Dim, ...]] = NotSpecified,
              scale: bool = NotSpecified,
              bias: bool = NotSpecified,
              epsilon: float = NotSpecified,
              name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Normalize over specified axes, e.g. time and/or feature axis.

  Note: For calculating a norm, see :class:`MathNormLayer` instead.

  In case of just feature (``axes="F"``),
  this corresponds to `layer normalization <https://arxiv.org/abs/1607.06450>`__ (see :class:`LayerNormLayer`).
  In case of time and feature (``axes="TF"``) for a 3D input,
  or more general all except batch (``axes="except_batch"``),
  this corresponds to `group normalization <https://arxiv.org/abs/1803.08494>`__ with G=1,
  or non-standard layer normalization.
  (The definition of layer-normalization is not clear on what axes should be normalized over.
  In many other frameworks, the default axis is just the last axis,
  which is usually the feature axis.
  However, in certain implementations and models,
  it is also common to normalize over all axes except batch.)

  The statistics are calculated just on the input.
  There are no running statistics (in contrast to batch normalization, see :class:`BatchNormLayer`).

  For some discussion on the definition of layer-norm vs group-norm,
  also see
  `here <https://stats.stackexchange.com/questions/485550/is-group-norm-with-g-1-equiv-to-layer-norm>`__
  and `here <https://github.com/tensorflow/addons/issues/2143>`__.

  :param LayerRef source:
  :param Dim|list[Dim] axis: axis or axes over which the mean and variance are computed, e.g. "F" or "TF"
    axis or axes over which the mean and variance are computed, e.g. "F" or "TF"
  :param Dim|list[Dim]|tuple[Dim] param_shape: shape of the scale and bias parameters.
    You can also refer to (static) axes of the input, such as the feature-dim.
    This is also the default, i.e. a param-shape of [F], independent of the axes to normalize over.
  :param bool scale: add trainable scale parameters
  :param bool bias: add trainable bias parameters
  :param float epsilon: epsilon for numerical stability
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'param_shape': param_shape,
    'scale': scale,
    'bias': bias,
    'epsilon': epsilon,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'norm',
    'from': source,
    **args}, name=name or 'normalize')


# noinspection PyShadowingBuiltins,PyShadowingNames
def math_norm(
              source: LayerRef,
              *,
              p: Union[int, float],
              axis: Union[Dim, List[Dim]],
              name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calculates sum(abs(x) ** p) ** (1./p).

  :param LayerRef source:
  :param int|float p:
  :param Dim|list[Dim] axis:
  :param str|NameCtx|None name:
  """
  args = {
    'p': p,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'math_norm',
    'from': source,
    **args}, name=name or 'math_norm')


# noinspection PyShadowingBuiltins,PyShadowingNames
def slice(
          source: LayerRef,
          *,
          axis: Dim,
          slice_start: Optional[int] = NotSpecified,
          slice_end: Optional[int] = NotSpecified,
          slice_step: Optional[int] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Slicing on the input, i.e. x[start:end:step] in some axis.
  See also :class:`SliceNdLayer`, for variable start.
  See also :class:`GatherLayer`, for one single position.

  Note that __getitem__ on a TF tensor (or also Numpy ND array) is more generic,
  and supports slices in multiple axes, as well as adding new dimensions, etc.
  It even allows to get boolean values, and then applies a boolean mask.
  See TF _slice_helper (== tf.Tensor.__getitem__) for a generic implementation,
  which calls tf.strided_slice.
  If we ever need such more generic support, we might consider adding a new layer,
  like ``GenericSliceLayer``, which gets a ``splice_spec``,
  just like ``_slice_helper`` (argument to ``__getitem__``).
  But any such a slice can already be constructed with multiple individual layers,
  which perform individual slices (per axis).

  We just support slicing in a single axis here, with optional striding (slice_step).

  :param LayerRef source:
  :param Dim axis:
  :param int|None slice_start:
  :param int|None slice_end:
  :param int|None slice_step:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'slice_start': slice_start,
    'slice_end': slice_end,
    'slice_step': slice_step,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'slice',
    'from': source,
    **args}, name=name or 'slice')


# noinspection PyShadowingBuiltins,PyShadowingNames
def slice_nd(
             source: LayerRef,
             *,
             start: LayerRef,
             size: Union[Dim, LayerRef],
             min_size: Optional[int] = NotSpecified,
             out_spatial_dim: Optional[Dim] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This takes out a slice-range from the time axis,
  e.g. ``x[start:start + size]``.
  If the input is of shape (B,T,F) and start is of shape (B,),
  then the output will be of shape (B,size,F).
  If the input is of shape (B,T,F) and start is of shape (B,T),
  then the output will be of shape (B,T,size,F).
  This layer allows a different start slice point for each batch,
  in contrast to :class:`SliceLayer`, and the start is variable.
  See also :class:`GatherNdLayer`.
  :class:`PrefixInTimeLayer` can recover the original shape (by zero-padding).

  :param LayerRef source:
  :param LayerBase start: (B,...)
  :param Dim|LayerBase size:
    We assume that this is >=0. If this might not be the case, use ``min_size=0``.
    If None, it uses the max possible size, and it becomes a dynamic axis.
  :param int|None min_size: if size is None, but we want to have a min-size
  :param Dim|None out_spatial_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'start': start,
    'size': size,
    'min_size': min_size,
    'out_spatial_dim': out_spatial_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'slice_nd',
    'from': source,
    **args}, name=name or 'slice_nd')


# noinspection PyShadowingBuiltins,PyShadowingNames
def gather(
           source: LayerRef,
           *,
           position: Union[LayerRef, int],
           axis: Dim,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gathers slices on a specified axis from the input layer using indices from a ``position`` layer.
  If the input is a layer of the shape ``[B,D,F1]``, and position of shape ``[B,F2]``, this will yield output of the
  shape ``[B,F2,F1]`` where

  ``output[b,f2,f1] = input[b,position[b,f2],f1]``

  (if ``D`` is the axis to gather from).
  In general, all shared axes of the input and the positions will be considered as batch-axes.

  The ``position`` argument can also be an ``int``.
  In this case, this simply gives ``input[position]`` one the specified ``axis``.

  It's basically a wrapper around ``tf.gather``.
  It provides the same functionality as the deprecated ``GatherNdLayer``, but is more generic.
  See also :class:`GatherNdLayer`.

  :param LayerRef source:
  :param LayerBase|int position: Layer containing the indices used to select the slices of the input from.
    If another layer, must be of type ``int32`` or ``int64``.
    Can also specify a constant ``int``.
  :param Dim axis: The axis into which we gather the indices into
  :param str|NameCtx|None name:
  """
  args = {
    'position': position,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'gather',
    'from': source,
    **args}, name=name or 'gather')


# noinspection PyShadowingBuiltins,PyShadowingNames
def scatter_nd(
               source: LayerRef,
               *,
               position: LayerRef,
               position_axis: Union[str, int],
               output_dim_via_time_from: Optional[LayerRef] = NotSpecified,
               out_spatial_dim: Optional[Dim] = NotSpecified,
               filter_invalid_indices: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  The inverse of :class:`GatherNdLayer`.
  Mostly a wrapper for ``tf.scatter_nd``.

  Note that "nd" is maybe a bit misleading.
  While we operate on N-D tensors, the indices (``position``)
  are into a single new dimension.

  The input to the layer are the ``updates``, the ``indices`` are via the ``position`` argument.
  The indices are into the newly constructed output dimension.
  The output shape is constructed via the common shape of the input, the position,
  and the unique common axis (if not unique, we would need to introduce an option to specify it)
  is replaced by the given output dimension (currently via ``output_dim_via_time_from``).

  Examples::

    position (indices): (B,eTs)
    input (updates): (eTs,D) or (B,eTs,D) -> expanded to (B,eTs,D)
    output shape: (B,eT,D)

    position (indices): (B,dT,eTs)
    input (updates): (eTs,D) -> expanded to (B,dT,eTs,D)
    output shape: (B,dT,eT,D)

    position (indices): (dT,eTs)
    input (updates): (eTs,D) -> expanded to (dT,eTs,D)
    output shape: (dT,eTs,D)

    position (indices): (dT,eTs)
    input (updates): (B,eTs,D) -> expanded to (dT,eTs,B,D)
    output shape: (dT,eT,B,D)

  In all these examples, output_dim_via_time_from is (B,eT,F), and eTs gets replaced by eT.

  :param LayerRef source:
  :param LayerBase position: indices into first axis (excluding batch) of the output
  :param str|int position_axis: axis in `position` to replace by the output-dim
  :param LayerBase|None output_dim_via_time_from: use the time-dim from this layer as the output-dim
  :param Dim|None out_spatial_dim:
  :param bool filter_invalid_indices: allow for indices <0 or >= output_dim, which will be discarded in the output
  :param str|NameCtx|None name:
  """
  args = {
    'position': position,
    'position_axis': position_axis,
    'output_dim_via_time_from': output_dim_via_time_from,
    'out_spatial_dim': out_spatial_dim,
    'filter_invalid_indices': filter_invalid_indices,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'scatter_nd',
    'from': source,
    **args}, name=name or 'scatter_nd')


class Linear(_Base):
  """
  Linear/forward/fully-connected/1x1-conv layer.
  Does a linear transformation on the feature-dimension of the input
  with an optional bias term and an optional activation function.
  See also :class:`DotLayer`, :class:`ElemwiseProdLayer`, :class:`WeightedSumLayer`.
  """
  returnn_layer_class = 'linear'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               out_dim: Dim,
               *,
               with_bias: bool = NotSpecified,
               grad_filter: Optional[float] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               bias_init: Union[str, float] = NotSpecified,
               use_transposed_weights: bool = NotSpecified,
               **kwargs):
    """
    :param Dim out_dim: output feature dimension
    :param bool with_bias:
    :param float|None grad_filter: if grad norm is higher than this threshold (before activation), the grad is removed
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str|float bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param bool use_transposed_weights: If True, define the weight matrix with transposed dimensions (n_out, n_in).
    """
    super().__init__(**kwargs)
    self.out_dim = out_dim
    self.with_bias = with_bias
    self.grad_filter = grad_filter
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.use_transposed_weights = use_transposed_weights

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'out_dim': self.out_dim,
      'with_bias': self.with_bias,
      'grad_filter': self.grad_filter,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'use_transposed_weights': self.use_transposed_weights,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef,
               ) -> Layer:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    return make_layer({
      'class': 'linear',
      'from': source,
      **self.get_opts()}, module=self)


# noinspection PyShadowingBuiltins,PyShadowingNames
def length(
           source: LayerRef,
           *,
           axis: Dim,
           dtype: str = NotSpecified,
           sparse: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Returns the length of sources as (B,), via input size_placeholder.

  :param LayerRef source:
  :param Dim axis:
  :param str dtype:
  :param bool sparse:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'dtype': dtype,
    'sparse': sparse,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'length',
    'from': source,
    **args}, name=name or 'length')


# noinspection PyShadowingBuiltins,PyShadowingNames
def softmax(
            source: LayerRef,
            *,
            axis: Dim,
            energy_factor: Optional[float] = NotSpecified,
            start: Optional[LayerRef] = NotSpecified,
            window_start: Optional[Union[LayerRef, int]] = NotSpecified,
            window_size: Optional[Union[LayerRef, int]] = NotSpecified,
            use_time_mask: bool = NotSpecified,
            log_space: bool = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This applies a softmax over spatial axis/axes (currently only time axis supported).
  E.g. when the input is of shape (B,T,dim), the output will be (B,T,dim).
  It automatically masks the frames outside the seq defined by the seq-len.
  In contrast to :class:`SoftmaxLayer`, this will not do a linear transformation.
  See :class:`SeqLenMaskLayer` if you just want to apply a masking.

  :param LayerRef source:
  :param Dim axis: which axis to do the softmax over. "T" by default
  :param float|None energy_factor: the energy will be scaled by this factor.
    This is like a temperature for the softmax.
    In Attention-is-all-you-need, this is set to 1/sqrt(base_ctx.dim).
  :param LayerBase|None start: Tensor of shape (B,) indicating the start frame
  :param LayerBase|int|None window_start: Layer with output of shape (B,) or (constant) int value indicating
    the window start.
  :param LayerBase|int|None window_size: Layer with output of shape (B,) or (constant) int value indicating
    the window size.
  :param bool use_time_mask: if True, assumes dyn seq len, and use it for masking.
    By default, if dyn seq len exists, it uses it.
  :param bool log_space: if True, returns in log space (i.e. uses log_softmax)
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'energy_factor': energy_factor,
    'start': start,
    'window_start': window_start,
    'window_size': window_size,
    'use_time_mask': use_time_mask,
    'log_space': log_space,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'softmax_over_spatial',
    'from': source,
    **args}, name=name or 'softmax')


# noinspection PyShadowingBuiltins,PyShadowingNames
def seq_len_mask(
                 source: LayerRef,
                 *,
                 mask_value: float,
                 axis: Dim,
                 seq_len_source: Optional[LayerRef] = NotSpecified,
                 start: Optional[LayerRef] = NotSpecified,
                 window_start: Optional[LayerRef] = NotSpecified,
                 window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                 name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Masks some values away given the seq_len_source with mask_value.
  Also see :class:`SoftmaxOverSpatialLayer`.
  Also see :class:`SwitchLayer`, which can be used to apply a generic mask.

  :param LayerRef source:
  :param float mask_value:
  :param Dim axis:
  :param LayerBase|None seq_len_source: if not given, uses source
  :param LayerBase|None start: Tensor of shape (B,) indicating the start frame
  :param LayerBase|None window_start: Tensor of shape (B,) indicating the window start
  :param LayerBase|int|None window_size:
  :param str|NameCtx|None name:
  """
  args = {
    'mask_value': mask_value,
    'axis': axis,
    'seq_len_source': seq_len_source,
    'start': start,
    'window_start': window_start,
    'window_size': window_size,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'seq_len_mask',
    'from': source,
    **args}, name=name or 'seq_len_mask')


# noinspection PyShadowingBuiltins,PyShadowingNames
def rand_int(
             source: LayerRef,
             *,
             shape: Union[Tuple[Dim, ...], List[Dim]],
             maxval: int,
             minval: int = NotSpecified,
             dtype: str = NotSpecified,
             sparse_dim: Optional[Dim] = NotSpecified,
             seed: Optional[int] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Generates random numbers using ``tf.random.uniform``

  :param LayerRef source:
  :param tuple[Dim]|list[Dim] shape: desired shape of output tensor
  :param int maxval: upper bound (exclusive) on range of random values
  :param int minval: lower bound (inclusive) on range of random values
  :param str dtype: type of the output. For random ints, int32 and int64 make sense, but could also be floats
  :param Dim|None sparse_dim:
  :param int|None seed: random seed
  :param str|NameCtx|None name:
  """
  args = {
    'shape': shape,
    'maxval': maxval,
    'minval': minval,
    'dtype': dtype,
    'sparse_dim': sparse_dim,
    'seed': seed,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'rand_int',
    'from': source,
    **args}, name=name or 'rand_int')


# noinspection PyShadowingBuiltins,PyShadowingNames
def range(
          source: LayerRef,
          *,
          limit: Union[int, float],
          start: Union[int, float] = NotSpecified,
          delta: Union[int, float] = NotSpecified,
          dtype: Optional[str] = NotSpecified,
          sparse: bool = NotSpecified,
          out_spatial_dim: Optional[Dim] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Generic wrapper around ``tf.range``.
  See also :class:`RangeInAxisLayer`.

  :param LayerRef source:
  :param int|float limit:
  :param int|float start:
  :param int|float delta:
  :param str|None dtype:
  :param bool sparse:
  :param Dim|None out_spatial_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'limit': limit,
    'start': start,
    'delta': delta,
    'dtype': dtype,
    'sparse': sparse,
    'out_spatial_dim': out_spatial_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'range',
    'from': source,
    **args}, name=name or 'range')


# noinspection PyShadowingBuiltins,PyShadowingNames
def range_in_axis(
                  source: LayerRef,
                  *,
                  axis: Dim,
                  dtype: str = NotSpecified,
                  sparse: bool = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Assume that the input is e.g. (B,T,D), and you specify axis="T", you will get (B=1,T,D=1),
  where the specified axis is filled with ``tf.range``.
  See also :class:`RangeLayer`.

  :param LayerRef source:
  :param Dim axis:
  :param str dtype:
  :param bool sparse:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'dtype': dtype,
    'sparse': sparse,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'range_in_axis',
    'from': source,
    **args}, name=name or 'range_in_axis')


# noinspection PyShadowingBuiltins,PyShadowingNames
def range_from_length(
                      source: LayerRef,
                      *,
                      dtype: str = NotSpecified,
                      sparse: bool = NotSpecified,
                      out_spatial_dim: Optional[Dim] = NotSpecified,
                      name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Given some dynamic sequence lengths as input, this creates a tf.range over the implied dimension.
  As a side effect, this can create a new dyn dim tag for the given sequence lengths.
  This side effect can be the main functionality in certain use cases.
  See also :class:`RangeInAxisLayer`.

  Consider the example::

    y: {class: range_in_axis, from: x, axis: T}

  This is basically equivalent to::

    x_len: {class: length, from: x}
    y: {class: range_from_length, from: x_len}


  :param LayerRef source:
  :param str dtype:
  :param bool sparse:
  :param Dim|None out_spatial_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'dtype': dtype,
    'sparse': sparse,
    'out_spatial_dim': out_spatial_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'range_from_length',
    'from': source,
    **args}, name=name or 'range_from_length')


# noinspection PyShadowingBuiltins,PyShadowingNames
def batch_softmax(
                  source: LayerRef,
                  *,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Softmax over spacial and feature axis

  :param LayerRef source:
  :param str|NameCtx|None name:
  """
  return make_layer({
    'class': 'batch_softmax',
    'from': source,
    }, name=name or 'batch_softmax')


# noinspection PyShadowingBuiltins,PyShadowingNames
def constant(
             *,
             value: Union[int, float, bool] = NotSpecified,
             shape: Union[Tuple[Dim, ...], List[Dim]] = NotSpecified,
             dtype: Optional[str] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Output is a constant value.

  :param int|float|bool value:
  :param tuple[Dim]|list[Dim] shape: for verification, and defining dim tags
  :param str|None dtype:
  :param str|NameCtx|None name:
  """
  args = {
    'value': value,
    'shape': shape,
    'dtype': dtype,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'constant',
    **args}, name=name or 'constant')


# noinspection PyShadowingBuiltins,PyShadowingNames
def window(
           source: LayerRef,
           *,
           state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           window_dim: Optional[Dim] = NotSpecified,
           window_left: Optional[int] = NotSpecified,
           window_right: Optional[int] = NotSpecified,
           axis: Dim,
           out_spatial_dim: Optional[Dim] = NotSpecified,
           padding: str = NotSpecified,
           stride: int = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  Adds a window dimension.
  By default, uses the time axis and goes over it with a sliding window.
  The new axis for the window is created right after the time axis.
  Will always return as batch major mode.
  E.g. if the input is (batch, time, dim), the output is (batch, time, window_size, dim).
  If you want to merge the (window_size, dim) together to (window_size * dim,),
  you can use the MergeDimsLayer, e.g. {"class": "merge_dims", "axes": "except_time"}.
  Use stride==window_size and window_right=window_size - 1 in combination with a
  MergeDimsLayer to achieve feature stacking with right-hand zero padding.

  This is not to take out a window from the time-dimension.
  See :class:`SliceLayer` or :class:`SliceNdLayer`.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param Dim|None window_dim:
  :param int|None window_left:
  :param int|None window_right:
  :param Dim axis: see :func:`Data.get_axis_from_description`
  :param Dim|None out_spatial_dim:
  :param str padding: "same" or "valid"
  :param int stride: return only each Nth window
  :param str|NameCtx|None name:
  """
  args = {
    'window_dim': window_dim,
    'window_left': window_left,
    'window_right': window_right,
    'axis': axis,
    'out_spatial_dim': out_spatial_dim,
    'padding': padding,
    'stride': stride,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'window',
    'from': source,
    **args}, name=name or 'window')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def cumsum(
           source: LayerRef,
           *,
           state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           axis: Dim,
           additional_left_summand_per_element: Optional[Union[str, int, float]] = NotSpecified,
           reverse: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  Basically wraps tf.cumsum. Also supports that in the RecLayer.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param Dim axis: see :func:`Data.get_axis_from_description`
  :param str|int|float|None additional_left_summand_per_element: the order matters for tf.string
  :param bool reverse:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'additional_left_summand_per_element': additional_left_summand_per_element,
    'reverse': reverse,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'cumsum',
    'from': source,
    **args}, name=name or 'cumsum')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def pad(
        source: LayerRef,
        *,
        axes: Union[Dim, List[Dim]],
        padding: Union[List[int, int], int, int, int],
        out_dims: Optional[Union[Dim, List[Dim]]] = NotSpecified,
        value: Union[int, float] = NotSpecified,
        mode: str = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds (e.g. zero) padding in some axis or axes.
  Also see :class:`PrefixInTimeLayer` for dynamic dims.

  :param LayerRef source:
  :param Dim|list[Dim] axes: e.g. "F" etc. see :func:`Data.get_axes_from_description`.
  :param list[(int,int)]|(int,int)|int padding: how much to pad left/right in each axis
  :param Dim|list[Dim]|None out_dims:
  :param int|float value: what constant value to pad, with mode=="constant"
  :param str mode: "constant", "reflect", "symmetric" and "replication"
  :param str|NameCtx|None name:
  """
  args = {
    'axes': axes,
    'padding': padding,
    'out_dims': out_dims,
    'value': value,
    'mode': mode,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'pad',
    'from': source,
    **args}, name=name or 'pad')


# noinspection PyShadowingBuiltins,PyShadowingNames
def merge_dims(
               source: LayerRef,
               *,
               axes: Sequence[Dim],
               out_dim: Optional[Dim] = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Merges a list of axes into a single one. (Flatten the dims.)
  E.g. input is (batch, width, height, dim) and axes=(1,2), then we get (batch, width*height, dim).
  Or input is (batch, time, height, dim) and axes="except_time", then we get (batch, time, height*dim).
  See also :class:`CombineDimsLayer`.
  When batch and time got merged, :class:`SplitBatchTimeLayer` can undo this.
  When you want to merge batch and time, but remove the padding efficiently, i.e. flatten it,
  see :class:`FlattenBatchLayer`.

  :param LayerRef source:
  :param Sequence[Dim] axes: see :func:`Data.get_axis_from_description`
  :param Dim|None out_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'axes': axes,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'merge_dims',
    'from': source,
    **args}, name=name or 'merge_dims')


# noinspection PyShadowingBuiltins,PyShadowingNames
def _split(
           source: LayerRef,
           *,
           axis: Dim,
           out_dims: Optional[List[Dim]] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Splits one axis into multiple parts, via tf.split.
  self.output is simply the input copied.
  Each part can be accessed via the sublayers "/%i".

  :param LayerRef source:
  :param Dim axis: feature axis by default
  :param list[Dim]|None out_dims:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'out_dims': out_dims,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'split',
    'from': source,
    **args}, name=name or 'split')


# noinspection PyShadowingBuiltins,PyShadowingNames
def split_dims(
               source: LayerRef,
               *,
               axis: Dim,
               dims: Union[Tuple[Dim, ...], List[Dim]],
               pad_to_multiples: Optional[bool] = NotSpecified,
               pad_value: Union[int, float] = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Splits one axis into multiple axes.
  E.g. if you know that your feature-dim is composed by a window,
  i.e. the input is (batch, time, window * feature),
  you can set axis="F", dims=(window, -1),
  and you will get the output (batch, time, window, feature).

  If the split axis has a dynamic length,
  exactly one of the axes that we split into need to also have a dynamic length.
  You can e.g. use this to split the input dimension into smaller "chunks" of a fixed window size.
  E.g. you could have input (batch, time, feature) and set axis="T", dims=(-1, window),
  to get output (batch, split_time, window, feature).
  In this case, the exact sequence lengths are lost and everything is padded to multiples of the window size using
  the given padding value.
  Use :class:`ReinterpretDataLayer` to receive back the original sequence lengths after merging.

  Also see :class:`SplitBatchTimeLayer`.
  Also see :class:`MergeDimsLayer` which can undo this operation.

  :param LayerRef source:
  :param Dim axis: e.g. "F"
  :param tuple[Dim]|list[Dim] dims: what the axis should be split into. e.g. (window, -1)
  :param bool|None pad_to_multiples: If true, input will be padded to the next multiple of the product of the
    static dims, such that splitting is actually possible.
    By default this is done iff the axis has a dynamic size
  :param int|float pad_value: What pad value to use for pad_to_multiples
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'dims': dims,
    'pad_to_multiples': pad_to_multiples,
    'pad_value': pad_value,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'split_dims',
    'from': source,
    **args}, name=name or 'split_dims')


# noinspection PyShadowingBuiltins,PyShadowingNames
def split_batch_time(
                     source: LayerRef,
                     *,
                     base: LayerRef,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A very specific layer which expects to get input of shape (batch * time, ...)
  and converts it into (batch, time, ...), where it recovers the seq-lens from some other layer.
  See :class:`SplitDimsLayer` for a more generic layer.

  :param LayerRef source:
  :param LayerBase base: used to recover the seq-lens
  :param str|NameCtx|None name:
  """
  args = {
    'base': base,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'split_batch_time',
    'from': source,
    **args}, name=name or 'split_batch_time')


# noinspection PyShadowingBuiltins,PyShadowingNames
def flatten_batch(
                  source: LayerRef,
                  *,
                  axis: Dim,
                  batch_major: bool = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Merges one axis into the batch axis.
  If the axis has dynamic lengths, this would use flattening,
  i.e. recalculate the padding, i.e. the size changes.
  This basically wraps :func:`flatten_with_seq_len_mask` or :func:`flatten_with_seq_len_mask_time_major`.
  See also :class:`MergeDimsLayer`, which does not do flattening,
  i.e. the size stays the same.

  :param LayerRef source:
  :param Dim axis:
  :param bool batch_major: if False, will flatten in time-major manner
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'batch_major': batch_major,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'flatten_batch',
    'from': source,
    **args}, name=name or 'flatten_batch')


# noinspection PyShadowingBuiltins,PyShadowingNames
def unflatten_batch(
                    source: LayerRef,
                    *,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Inverse of :class:`FlattenBatchLayer`, so recovers an axis previously merged into the batch axis

  This basically wraps :func:`unflatten_with_seq_len_mask`.

  :param LayerRef source:
  :param str|NameCtx|None name:
  """
  return make_layer({
    'class': 'unflatten_batch',
    'from': source,
    }, name=name or 'unflatten_batch')


# noinspection PyShadowingBuiltins,PyShadowingNames
def unflatten_nd(
                 source: LayerRef,
                 *,
                 sizes: LayerRef,
                 num_axes: int,
                 in_dim: Optional[Dim] = NotSpecified,
                 out_dims: Optional[List[Dim]] = NotSpecified,
                 declare_same_sizes_as: Optional[Dict[int, LayerRef]] = NotSpecified,
                 name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This keeps the batch axis as-is, i.e. the flattening/unflattening did not happen on the batch axis.

  Example:

    Assumes that the input is of shape (B,T,<Ds>) which represents flattened images,
    where each image is of size width * height.
    We additionally provide these image sizes (shape (B,2)), i.e. (width,height) tuples.
    We return the unflattened images of shape (B,W,H,<Ds>), where W/H are the max width/height.

  This basically wraps :func:`returnn.tf.util.basic.unflatten_nd`.

  :param LayerRef source:
  :param LayerBase sizes:
  :param int num_axes:
  :param Dim|None in_dim:
  :param list[Dim]|None out_dims:
  :param dict[int,LayerBase]|None declare_same_sizes_as:
  :param str|NameCtx|None name:
  """
  args = {
    'sizes': sizes,
    'num_axes': num_axes,
    'in_dim': in_dim,
    'out_dims': out_dims,
    'declare_same_sizes_as': declare_same_sizes_as,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'unflatten_nd',
    'from': source,
    **args}, name=name or 'unflatten_nd')


# noinspection PyShadowingBuiltins,PyShadowingNames
def repeat(
           source: LayerRef,
           *,
           repetitions: Union[LayerRef, int],
           axis: Dim,
           out_dim: Optional[Dim] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A wrapper around tf.repeat, but supports an additional batch axis for the durations
  The sum of the repetitions has to be non-zero for each sequence in the batch.

  This layer can only be used with Tensorflow 1.15.0 or newer.

  :param LayerRef source:
  :param LayerBase|int repetitions:
    number of repetitions for each sequence and position in target axis.
    Can be [B,T] or [T,B] or some subset of that shape
  :param Dim axis: (dynamic) axis for repetition (currently only time axis is supported)
  :param Dim|None out_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'repetitions': repetitions,
    'axis': axis,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'repeat',
    'from': source,
    **args}, name=name or 'repeat')


# noinspection PyShadowingBuiltins,PyShadowingNames
def tile(
         source: LayerRef,
         *,
         multiples: Dict[Dim,  int],
         out_dims: Optional[Dict[Dim,  Dim]] = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A wrapper around tf.tile

  :param LayerRef source:
  :param dict[Dim, int] multiples: number of multiples per axis (axis provided as dim tag or str desc)
  :param dict[Dim, Dim]|None out_dims:
  :param str|NameCtx|None name:
  """
  args = {
    'multiples': multiples,
    'out_dims': out_dims,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'tile',
    'from': source,
    **args}, name=name or 'tile')


# noinspection PyShadowingBuiltins,PyShadowingNames
def cast(
         source: LayerRef,
         *,
         dtype: str,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Cast to some other dtype.

  :param LayerRef source:
  :param str dtype:
  :param str|NameCtx|None name:
  """
  args = {
    'dtype': dtype,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'cast',
    'from': source,
    **args}, name=name or 'cast')


# noinspection PyShadowingBuiltins,PyShadowingNames
def reinterpret_data(
                     source: LayerRef,
                     *,
                     switch_axes: Union[str, List[str]] = NotSpecified,
                     size_base: Optional[LayerRef] = NotSpecified,
                     set_axes: Dict[str, Union[int, str]] = NotSpecified,
                     set_dim_tags: Optional[Dict[Dim,  Dim]] = NotSpecified,
                     set_sparse: Optional[bool] = NotSpecified,
                     set_sparse_dim: Optional[Union[Dim, NotSpecified]] = NotSpecified,
                     increase_sparse_dim: Optional[int] = NotSpecified,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Acts like the :class:`CopyLayer` but reinterprets the role of some axes or data.

  :param LayerRef source:
  :param str|list[str] switch_axes: e.g. "bt" to switch batch and time axes
  :param LayerBase|None size_base: copy the size_placeholder from the given layer
  :param dict[str,int|str] set_axes:
    This can be used to overwrite the special axes like time_dim_axis or feature_dim_axis.
    For that, use keys "B","T" or "F", and a value via :func:`Data.get_axis_from_description`.
  :param dict[Dim, Dim]|None set_dim_tags: axis -> new dim tag. assigns new dim tags.
    If the passed dim tag is yet undefined, this will not use same_dim_tags_as (declare_same_as)
    but create a new dim tag.
    This option is useful for generalized self attention (https://github.com/rwth-i6/returnn/issues/391).
  :param bool|None set_sparse: if bool, set sparse value to this
  :param Dim|NotSpecified|None set_sparse_dim: set sparse dim to this. assumes that it is sparse
  :param int|None increase_sparse_dim: add this to the dim. assumes that it is sparse
  :param str|NameCtx|None name:
  """
  args = {
    'switch_axes': switch_axes,
    'size_base': size_base,
    'set_axes': set_axes,
    'set_dim_tags': set_dim_tags,
    'set_sparse': set_sparse,
    'set_sparse_dim': set_sparse_dim,
    'increase_sparse_dim': increase_sparse_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'reinterpret_data',
    'from': source,
    **args}, name=name or 'reinterpret_data')


class Conv(_Base):
  """
  A generic convolution layer which supports 1D, 2D and 3D convolution.
  Pooling can be done in the separate "pool" layer.
  """
  returnn_layer_class = 'conv'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               out_dim: Optional[Dim] = NotSpecified,
               filter_size: Tuple[int, ...],
               padding: str,
               strides: Union[int, Tuple[int, ...]] = NotSpecified,
               dilation_rate: Union[int, Tuple[int, ...]] = NotSpecified,
               groups: int = NotSpecified,
               input_expand_dims: int = NotSpecified,
               input_add_feature_dim: bool = NotSpecified,
               input_split_feature_dim: Optional[int] = NotSpecified,
               in_dim: Optional[Dim] = NotSpecified,
               in_spatial_dims: Optional[List[Dim]] = NotSpecified,
               out_spatial_dims: Optional[List[Dim]] = NotSpecified,
               auto_use_channel_first: Union[bool, NotSpecified] = NotSpecified,
               with_bias: Union[bool, NotSpecified] = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param Dim|None out_dim:
    :param tuple[int] filter_size: (width,), (height,width) or (depth,height,width) for 1D/2D/3D conv.
      the input data ndim must match, or you can add dimensions via input_expand_dims or input_add_feature_dim.
      it will automatically swap the batch-dim to the first axis of the input data.
    :param str padding: "same" or "valid"
    :param int|tuple[int] strides: strides for the spatial dims,
      i.e. length of this tuple should be the same as filter_size, or a single int.
    :param int|tuple[int] dilation_rate: dilation for the spatial dims
    :param int groups: grouped convolution
    :param int input_expand_dims: number of spatial dims to add to the input
    :param bool input_add_feature_dim: will add a dim at the end and use input-feature-dim == 1,
      and use the original input feature-dim as a spatial dim.
    :param None|int input_split_feature_dim: if set, like input_add_feature_dim it will add a new feature dim
      which is of value input_split_feature_dim, and the original input feature dim
      will be divided by input_split_feature_dim, thus it must be a multiple of that value.
    :param Dim|None in_dim:
    :param list[Dim]|None in_spatial_dims:
    :param list[Dim]|None out_spatial_dims:
    :param bool|NotSpecified auto_use_channel_first: convert the input to NCHW or not
    :param bool|NotSpecified with_bias: if True, will add a bias to the output features. False by default
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.out_dim = out_dim
    self.filter_size = filter_size
    self.padding = padding
    self.strides = strides
    self.dilation_rate = dilation_rate
    self.groups = groups
    self.input_expand_dims = input_expand_dims
    self.input_add_feature_dim = input_add_feature_dim
    self.input_split_feature_dim = input_split_feature_dim
    self.in_dim = in_dim
    self.in_spatial_dims = in_spatial_dims
    self.out_spatial_dims = out_spatial_dims
    self.auto_use_channel_first = auto_use_channel_first
    self.with_bias = with_bias
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'out_dim': self.out_dim,
      'filter_size': self.filter_size,
      'padding': self.padding,
      'strides': self.strides,
      'dilation_rate': self.dilation_rate,
      'groups': self.groups,
      'input_expand_dims': self.input_expand_dims,
      'input_add_feature_dim': self.input_add_feature_dim,
      'input_split_feature_dim': self.input_split_feature_dim,
      'in_dim': self.in_dim,
      'in_spatial_dims': self.in_spatial_dims,
      'out_spatial_dims': self.out_spatial_dims,
      'auto_use_channel_first': self.auto_use_channel_first,
      'with_bias': self.with_bias,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef,
               *,
               filter: Optional[LayerRef] = NotSpecified,
               bias: Optional[LayerRef] = NotSpecified,
               ) -> Layer:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return make_layer({
      'class': 'conv',
      'from': source,
      **args,
      **self.get_opts()}, module=self)


# noinspection PyShadowingBuiltins,PyShadowingNames
def pool(
         source: LayerRef,
         *,
         mode: str,
         pool_size: Tuple[int, ...],
         padding: str = NotSpecified,
         dilation_rate: Union[Tuple[int, ...], int] = NotSpecified,
         strides: Optional[Union[Tuple[int, ...], int]] = NotSpecified,
         in_dim: Optional[Dim] = NotSpecified,
         in_spatial_dims: Optional[List[Dim]] = NotSpecified,
         out_dim: Optional[Dim] = NotSpecified,
         out_spatial_dims: Optional[List[Dim]] = NotSpecified,
         use_channel_first: Union[bool, NotSpecified] = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A generic N-D pooling layer.
  This would usually be done after a convolution for down-sampling.

  :param LayerRef source:
  :param str mode: "max" or "avg"
  :param tuple[int] pool_size: shape of the window of each reduce
  :param str padding: "valid" or "same"
  :param tuple[int]|int dilation_rate:
  :param tuple[int]|int|None strides: in contrast to tf.nn.pool, the default (if it is None) will be set to pool_size
  :param Dim|None in_dim:
  :param list[Dim]|None in_spatial_dims:
  :param Dim|None out_dim:
  :param list[Dim]|None out_spatial_dims:
  :param bool|NotSpecified use_channel_first: if set, will transform input to NCHW format
  :param str|NameCtx|None name:
  """
  args = {
    'mode': mode,
    'pool_size': pool_size,
    'padding': padding,
    'dilation_rate': dilation_rate,
    'strides': strides,
    'in_dim': in_dim,
    'in_spatial_dims': in_spatial_dims,
    'out_dim': out_dim,
    'out_spatial_dims': out_spatial_dims,
    'use_channel_first': use_channel_first,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'pool',
    'from': source,
    **args}, name=name or 'pool')


# noinspection PyShadowingBuiltins,PyShadowingNames
def dct(
        source: LayerRef,
        *,
        type: int = NotSpecified,
        n: Optional[int] = NotSpecified,
        norm: Optional[str] = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Layer to perform DCT
  Wraps :func:`tf.signal.dct`. For further documentation on the input arguments, refer to
  https://www.tensorflow.org/api_docs/python/tf/signal/dct

  :param LayerRef source:
  :param int type: DCT type to perform. Must be 1, 2, 3, or 4
  :param int|None n: length of the transform
  :param str|None norm: normalization to apply. Must be None or "ortho"
  :param str|NameCtx|None name:
  """
  args = {
    'type': type,
    'n': n,
    'norm': norm,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'dct',
    'from': source,
    **args}, name=name or 'dct')


class TransposedConv(_Base):
  """
  Transposed convolution, sometimes also called deconvolution.
  See :func:`tf.nn.conv2d_transpose` (currently we support 1D/2D).
  """
  returnn_layer_class = 'transposed_conv'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               out_dim: Optional[Dim] = NotSpecified,
               filter_size: List[int],
               strides: Optional[List[int]] = NotSpecified,
               padding: str = NotSpecified,
               remove_padding: Union[List[int], int] = NotSpecified,
               output_padding: Optional[Union[List[Optional[int]], int]] = NotSpecified,
               in_dim: Optional[Dim] = NotSpecified,
               in_spatial_dims: Optional[List[Dim]] = NotSpecified,
               out_spatial_dims: Optional[List[Dim]] = NotSpecified,
               with_bias: bool = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param Dim|None out_dim:
    :param list[int] filter_size:
    :param list[int]|None strides: specifies the upscaling. by default, same as filter_size
    :param str padding: "same" or "valid"
    :param list[int]|int remove_padding:
    :param list[int|None]|int|None output_padding:
    :param Dim|None in_dim:
    :param list[Dim]|None in_spatial_dims:
    :param list[Dim]|None out_spatial_dims:
    :param bool with_bias: whether to add a bias. enabled by default.
      Note that the default is different from ConvLayer!
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.out_dim = out_dim
    self.filter_size = filter_size
    self.strides = strides
    self.padding = padding
    self.remove_padding = remove_padding
    self.output_padding = output_padding
    self.in_dim = in_dim
    self.in_spatial_dims = in_spatial_dims
    self.out_spatial_dims = out_spatial_dims
    self.with_bias = with_bias
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'out_dim': self.out_dim,
      'filter_size': self.filter_size,
      'strides': self.strides,
      'padding': self.padding,
      'remove_padding': self.remove_padding,
      'output_padding': self.output_padding,
      'in_dim': self.in_dim,
      'in_spatial_dims': self.in_spatial_dims,
      'out_spatial_dims': self.out_spatial_dims,
      'with_bias': self.with_bias,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef,
               *,
               filter: Optional[LayerRef] = NotSpecified,
               bias: Optional[LayerRef] = NotSpecified,
               ) -> Layer:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return make_layer({
      'class': 'transposed_conv',
      'from': source,
      **args,
      **self.get_opts()}, module=self)


# noinspection PyShadowingBuiltins,PyShadowingNames
def reduce(
           source: LayerRef,
           *,
           mode: str,
           axis: Union[Dim, List[Dim]],
           use_time_mask: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This reduces some axis by using "sum" or "max".
  It's basically a wrapper around tf.reduce_sum or tf.reduce_max.

  :param LayerRef source:
  :param str mode: "sum" or "max", "argmin", "min", "argmax", "mean", "logsumexp"
  :param Dim|list[Dim] axis: for compatibility, can be used instead of ``axes``
    One axis or multiple axis to reduce.
    It accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature",
    and it is strongly recommended to use some of these symbolic names.
    See :func:`Data.get_axes_from_description`.
  :param bool use_time_mask: if we reduce over the time-dim axis, use the seq len info.
    By default, in that case, it will be True.
  :param str|NameCtx|None name:
  """
  args = {
    'mode': mode,
    'axis': axis,
    'use_time_mask': use_time_mask,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'reduce',
    'from': source,
    **args}, name=name or 'reduce')


# noinspection PyShadowingBuiltins,PyShadowingNames
def reduce_out(
               source: LayerRef,
               *,
               mode: str,
               num_pieces: int,
               out_dim: Optional[Dim] = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Combination of :class:`SplitDimsLayer` applied to the feature dim
  and :class:`ReduceLayer` applied to the resulting feature dim.
  This can e.g. be used to do maxout.

  :param LayerRef source:
  :param str mode: "sum" or "max" or "mean"
  :param int num_pieces: how many elements to reduce. The output dimension will be input.dim // num_pieces.
  :param Dim|None out_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'mode': mode,
    'num_pieces': num_pieces,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'reduce_out',
    'from': source,
    **args}, name=name or 'reduce_out')


# noinspection PyShadowingBuiltins,PyShadowingNames
def squeeze(
            source: LayerRef,
            *,
            axis: Union[Dim, List[Dim]],
            allow_no_op: bool = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Removes an axis with dimension 1.
  This is basically a wrapper around tf.squeeze.

  :param LayerRef source:
  :param Dim|list[Dim] axis: one axis or multiple axis to squeeze.
    this is counted with batch-dim, which by default is axis 0 (see enforce_batch_dim_axis).
    it also accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature"
  :param bool allow_no_op:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'allow_no_op': allow_no_op,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'squeeze',
    'from': source,
    **args}, name=name or 'squeeze')


# noinspection PyShadowingBuiltins,PyShadowingNames
def stack(
          source: Union[List[LayerRef], Tuple[LayerRef]],
          *,
          axis: Dim,
          out_spatial_dim: Optional[Dim] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Stacks multiple inputs together using :func:`tf.stack`.
  This creates a new dimension for the stack.

  For concatenation (in feature dimension), see :class:`CopyLayer`.

  :param list[LayerRef]|tuple[LayerRef] source:
  :param Dim axis: new axis.
    If not given, will use Data.get_default_new_axis_for_dim_tag(<spatial>),
    i.e. some reasonable default for a new spatial axis.
  :param Dim|None out_spatial_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'out_spatial_dim': out_spatial_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'stack',
    'from': source,
    **args}, name=name or 'stack')


# noinspection PyShadowingBuiltins,PyShadowingNames
def prefix_in_time(
                   source: LayerRef,
                   *,
                   axis: Dim,
                   out_dim: Optional[Dim] = NotSpecified,
                   prefix: Union[float, str] = NotSpecified,
                   repeat: Union[int, LayerRef] = NotSpecified,
                   size_base: Optional[LayerRef] = NotSpecified,
                   name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds some prefix in time dimension.
  This is kind of the reverse of :class:`SliceNdLayer` does.
  Also see :class:`PadLayer` for static dimensions.
  Also see :class:`PostfixInTimeLayer`.

  :param LayerRef source:
  :param Dim axis:
  :param Dim|None out_dim:
  :param float|str prefix: either some constant or another layer
  :param int|LayerBase repeat: how often to repeat the prefix
  :param LayerBase|None size_base: copy seq-lens from here
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'out_dim': out_dim,
    'prefix': prefix,
    'repeat': repeat,
    'size_base': size_base,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'prefix_in_time',
    'from': source,
    **args}, name=name or 'prefix_in_time')


# noinspection PyShadowingBuiltins,PyShadowingNames
def postfix_in_time(
                    source: LayerRef,
                    *,
                    axis: Dim,
                    out_dim: Optional[Dim] = NotSpecified,
                    postfix: Union[float, int, LayerRef] = NotSpecified,
                    repeat: int = NotSpecified,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds some postfix in time dimension.
  Also see :class:`PrefixInTimeLayer`.

  :param LayerRef source:
  :param Dim axis:
  :param Dim|None out_dim:
  :param float|int|LayerBase postfix: constant or other layer without time axis to use as postfix
  :param int repeat: how often to repeat the postfix
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'out_dim': out_dim,
    'postfix': postfix,
    'repeat': repeat,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'postfix_in_time',
    'from': source,
    **args}, name=name or 'postfix_in_time')


# noinspection PyShadowingBuiltins,PyShadowingNames
def time_chunking(
                  source: LayerRef,
                  *,
                  chunk_size: int,
                  chunk_step: int,
                  axis: Dim,
                  out_dim: Optional[Dim] = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Performs chunking in time. See :func:`returnn.tf.native_op.chunk`.

  :param LayerRef source:
  :param int chunk_size:
  :param int chunk_step:
  :param Dim axis:
  :param Dim|None out_dim:
  :param str|NameCtx|None name:
  """
  args = {
    'chunk_size': chunk_size,
    'chunk_step': chunk_step,
    'axis': axis,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'time_chunking',
    'from': source,
    **args}, name=name or 'time_chunking')


# noinspection PyShadowingBuiltins,PyShadowingNames
def time_un_chunking(
                     source: LayerRef,
                     *,
                     chunking_layer: LayerRef,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.

  :param LayerRef source:
  :param TimeChunkingLayer chunking_layer:
  :param str|NameCtx|None name:
  """
  args = {
    'chunking_layer': chunking_layer,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'time_unchunking',
    'from': source,
    **args}, name=name or 'time_un_chunking')


# noinspection PyShadowingBuiltins,PyShadowingNames
def dot(
        source1: LayerRef,
        source2: LayerRef,
        *,
        reduce: Union[Dim, Tuple[Dim, ...], List[Dim]] = NotSpecified,
        debug: bool = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This performs a dot-product of two sources.
  The underlying matmul expects shapes (shared..., I, J) * (shared..., J, K) -> (shared..., I, K).
  We say that J is the axis to be reduced,
  I is the var-dim of source 1, and K is the var-dim of source 2.
  I, J, K can also be multiple axes from the sources.
  The var-dims don't need to exist.
  All other axes (shared...) are expected to match.

  Earlier defaults:
    red1=-1, red2=-2, var1=-2, var2=-1, add_var2_if_empty=True.
  However, these are bad, for multiple reasons, like using integers, but also in general.
    See https://github.com/rwth-i6/returnn/issues/627 for details.

  :param LayerRef source1:
  :param LayerRef source2:
  :param Dim|tuple[Dim]|list[Dim] reduce: reduce axes of both sources
  :param bool debug: will print debug shapes, etc.
  :param str|NameCtx|None name:
  """
  args = {
    'reduce': reduce,
    'debug': debug,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'dot',
    'from': [source1, source2],
    **args}, name=name or 'dot')


# noinspection PyShadowingBuiltins,PyShadowingNames
def shift_axis(
               source: LayerRef,
               *,
               axis: Dim,
               amount: int,
               pad: bool = NotSpecified,
               adjust_size_info: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Shifts the dimensions in an axis around by slicing and optional padding.
  This layer may change the axis-dimension.

  This name might be confusing. No axis will be shifted here. See :class:`SwapAxesLayer` for that.

  :param LayerRef source:
  :param Dim axis: single axis to shift
  :param int amount: number of elements to shift
                 (<0 for left-shift, >0 for right-shift)
  :param bool pad: preserve shape by padding
  :param bool adjust_size_info: whether to adjust the size_placeholder
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'amount': amount,
    'pad': pad,
    'adjust_size_info': adjust_size_info,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'shift_axis',
    'from': source,
    **args}, name=name or 'shift_axis')


# noinspection PyShadowingBuiltins,PyShadowingNames
def resize(
           source: LayerRef,
           *,
           factor: int,
           axis: Dim,
           out_dim: Optional[Dim] = NotSpecified,
           kind: str = NotSpecified,
           fill_value: Optional[Union[int, float]] = NotSpecified,
           fill_dropout: float = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Resizes the input, i.e. upsampling or downsampling.
  Supports different kinds, such as linear interpolation or nearest-neighbor.

  :param LayerRef source:
  :param int factor:
  :param Dim axis: the axis to resize
  :param Dim|None out_dim:
  :param str kind: "linear", "nn"/"nearest_neighbor", "cubic", "fill"
  :param None|int|float fill_value: if kind=="fill"
  :param float fill_dropout: if set, will dropout in the same axis
  :param str|NameCtx|None name:
  """
  args = {
    'factor': factor,
    'axis': axis,
    'out_dim': out_dim,
    'kind': kind,
    'fill_value': fill_value,
    'fill_dropout': fill_dropout,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'resize',
    'from': source,
    **args}, name=name or 'resize')


# noinspection PyShadowingBuiltins,PyShadowingNames
def remove(
           source: LayerRef,
           *,
           symbol: int,
           axis: Dim,
           out_dim: Optional[Dim] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Currently, assumes sparse data, and removes a specific symbol from the data.

  It is recommended to use :class:`MaskedComputationLayer` in combination with e.g.
  a :class:CompareLayer` instead, as this provides more flexibility.

  :param LayerRef source:
  :param int symbol:
  :param Dim axis: the axis to operate over, to potentially remove frames
  :param Dim|None out_dim: derived from the dim of axis, the reduced new dim
  :param str|NameCtx|None name:
  """
  args = {
    'symbol': symbol,
    'axis': axis,
    'out_dim': out_dim,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'remove',
    'from': source,
    **args}, name=name or 'remove')


# noinspection PyShadowingBuiltins,PyShadowingNames
def _combine(
             source: Union[List[LayerRef], Tuple[LayerRef]],
             *,
             kind: str,
             with_bias: bool = NotSpecified,
             eval: Union[str, callable] = NotSpecified,
             eval_locals: Optional[Dict[str]] = NotSpecified,
             eval_for_output_loss: bool = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Applies a binary operation, such as addition, to all sources while accumulating the partial results.
  In the first step, the binary operation is performed on the first two sources.
  After the first step, the previous results is always the left-hand operator.

  Its basic working is similar to the `reduce` function used in functional programming.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  :param list[LayerRef]|tuple[LayerRef] source:
  :param str kind:
    currently accepted values are `average`, `add`, `sub`, `mul`, `truediv`, `logical_and`, `logical_or`, or `eval`
  :param bool with_bias: if given, will add a trainable bias tensor
  :param str|callable eval: for kind="eval", will eval this string. or function. see :func:`_op_kind_eval`
  :param dict[str]|None eval_locals: locals for eval
  :param bool eval_for_output_loss: will do the same eval on layer.output_loss
  :param str|NameCtx|None name:
  """
  args = {
    'kind': kind,
    'with_bias': with_bias,
    'eval': eval,
    'eval_locals': eval_locals,
    'eval_for_output_loss': eval_for_output_loss,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'combine',
    'from': source,
    **args}, name=name or 'combine')


# noinspection PyShadowingBuiltins,PyShadowingNames
def _eval(
          source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
          *,
          eval: str,
          with_bias: bool = NotSpecified,
          eval_locals: Optional[Dict[str]] = NotSpecified,
          eval_for_output_loss: bool = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Evaluates some string.
  The :class:`CombineLayer` provides this functionality, thus this is just a special case of it.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  The output type is defined as a broadcasted extension of all sources.
  You can overwrite it by (partially) specifying `out_type`.
  `out_type` can also be a generic Python function, returning a `Data` instance.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str eval: will eval this string. see :func:`_op_kind_eval`
  :param bool with_bias: if given, will add a trainable bias tensor
  :param dict[str]|None eval_locals: locals for eval
  :param bool eval_for_output_loss: will do the same eval on layer.output_loss
  :param str|NameCtx|None name:
  """
  args = {
    'eval': eval,
    'with_bias': with_bias,
    'eval_locals': eval_locals,
    'eval_for_output_loss': eval_for_output_loss,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'eval',
    'from': source,
    **args}, name=name or 'eval')


# noinspection PyShadowingBuiltins,PyShadowingNames
def compare(
            source: Union[List[LayerRef], Tuple[LayerRef]],
            *,
            kind: str = NotSpecified,
            value: Optional[Union[float, int]] = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Compares element-wise the tokens of all input sequences among themselves and/or with a specified given value.
  The comparisons are performed in a chain according to the order in which they are listed.

  Example::

      {"class": "compare", "from": ["i1", "i2"], "value": val, "kind": "less"}

  computes i1 < i2 < val and it is true only if the whole chain of operations is true.
  The final result is the logical "and" of all comparisons. Note that `value` is the last element to be compared to.

  A common example usage is the `end` layer in a rec subnetwork to specify the stopping criterion,
  e.g. the last generated token is equal to the end-of-sentence token::

      "output": {"class": "rec", "from": [], "unit": {
          .
          .
          .
          "end": {"class": "compare", "from": "output", "value": end_of_sentence_id}
      }, "target": "classes0"}


  :param list[LayerRef]|tuple[LayerRef] source:
  :param str kind: which comparison operation to use, e.g. "equal", "greater", "less"
    or other supported TF comparison ops
  :param float|int|None value: if specified, will also compare to this
  :param str|NameCtx|None name:
  """
  args = {
    'kind': kind,
    'value': value,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'compare',
    'from': source,
    **args}, name=name or 'compare')


# noinspection PyShadowingBuiltins,PyShadowingNames
def switch(
           *,
           condition: Union[LayerRef, bool],
           true_from: Optional[Union[LayerRef, float, int]],
           false_from: Optional[Union[LayerRef, float, int]],
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Wrapper around ``tf.where()`` (or more generically :func:`returnn.tf.util.basic.where_bc`),
  or statically choose a single source if the condition is a callable (...)->bool.
  (``tf.cond`` is not useful here, as the sources would have been already constructed and computed.)

  This layer is also useful for applying any kind of generic masking to the frames.
  E.g. one could have a layer called "mask" computing a boolean mask for the values stored in another layer "input".
  Then use this layer with condition="mask", true_from="input", false_from=mask_value,
  to mask out all frames where the mask is false with the mask_value.

  See also :class:`CondLayer`.
  See also :class:`SeqLenMaskLayer` if you just want to mask using the sequence lengths.

  :param LayerBase|bool condition: if callable, expected to be (...)->bool, and called in transform_config_dict
  :param LayerBase|float|int|None true_from:
  :param LayerBase|float|int|None false_from:
  :param str|NameCtx|None name:
  """
  args = {
    'condition': condition,
    'true_from': true_from,
    'false_from': false_from,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'switch',
    **args}, name=name or 'switch')


# noinspection PyShadowingBuiltins,PyShadowingNames
def search_sorted(
                  source: LayerRef,
                  *,
                  sorted_sequence: LayerRef,
                  values: LayerRef,
                  axis: Dim,
                  side: str = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Basically wraps :func:`tf.searchsorted`.

  Takes a tensor `sorted_sequence` that is sorted along one axis, and a tensor `values`.
  Will compute an output tensor with the same axes as `values`,
  where each entry is the index of the value within the sorted sequence.
  All (batch) axes of `sorted_sequence` except for the axis it is sorted along must be present in `values`.

  :param LayerRef source:
  :param LayerBase sorted_sequence:
  :param LayerBase values: search values
  :param Dim axis: the axis along which `sorted_sequence` is sorted
  :param str side: "left" or "right".
    When one of the `values` exactly matches an element of the `sorted_sequence`,
    whether to choose the lower or higher index.
  :param str|NameCtx|None name:
  """
  args = {
    'sorted_sequence': sorted_sequence,
    'values': values,
    'axis': axis,
    'side': side,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'search_sorted',
    'from': source,
    **args}, name=name or 'search_sorted')


# noinspection PyShadowingBuiltins,PyShadowingNames
def variable(
             *,
             shape: Union[Tuple[Dim, ...], List[Dim]],
             dtype: str = NotSpecified,
             trainable: bool = NotSpecified,
             init: Union[str, float, int] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Represents a variable. Can add batch/time dimension if wanted. Can be trainable.
  See defaults.

  :param tuple[Dim]|list[Dim] shape:
  :param str dtype:
  :param bool trainable:
  :param str|float|int init: see :func:`returnn.tf.util.basic.get_initializer`
  :param str|NameCtx|None name:
  """
  args = {
    'shape': shape,
    'dtype': dtype,
    'trainable': trainable,
    'init': init,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'variable',
    **args}, name=name or 'variable')


# noinspection PyShadowingBuiltins,PyShadowingNames
def forced_alignment(
                     source: LayerRef,
                     *,
                     align_target: LayerRef,
                     topology: str,
                     input_type: str,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calculates a forced alignment, via Viterbi algorithm.

  :param LayerRef source:
  :param LayerBase align_target:
  :param str topology: e.g. "ctc" or "rna" (RNA is CTC without label loop)
  :param str input_type: "log_prob" or "prob"
  :param str|NameCtx|None name:
  """
  args = {
    'align_target': align_target,
    'topology': topology,
    'input_type': input_type,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'forced_align',
    'from': source,
    **args}, name=name or 'forced_alignment')


# noinspection PyShadowingBuiltins,PyShadowingNames
def fast_baum_welch(
                    source: LayerRef,
                    *,
                    align_target: str,
                    align_target_key: Optional[str] = NotSpecified,
                    ctc_opts: Dict[str] = NotSpecified,
                    sprint_opts: Dict[str] = NotSpecified,
                    input_type: str = NotSpecified,
                    tdp_scale: float = NotSpecified,
                    am_scale: float = NotSpecified,
                    min_prob: float = NotSpecified,
                    staircase_seq_len_source: Optional[LayerRef] = NotSpecified,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calls :func:`fast_baum_welch` or :func:`fast_baum_welch_by_sprint_automata`.
  We expect that our input are +log scores, e.g. use log-softmax.

  :param LayerRef source:
  :param str align_target: e.g. "sprint" or "staircase"
  :param str|None align_target_key: e.g. "classes", used for e.g. align_target "ctc"
  :param dict[str] ctc_opts: used for align_target "ctc"
  :param dict[str] sprint_opts: used for Sprint (RASR) for align_target "sprint"
  :param str input_type: "log_prob" or "prob"
  :param float tdp_scale:
  :param float am_scale:
  :param float min_prob: clips the minimum prob (value in [0,1])
  :param LayerBase|None staircase_seq_len_source:
  :param str|NameCtx|None name:
  """
  args = {
    'align_target': align_target,
    'align_target_key': align_target_key,
    'ctc_opts': ctc_opts,
    'sprint_opts': sprint_opts,
    'input_type': input_type,
    'tdp_scale': tdp_scale,
    'am_scale': am_scale,
    'min_prob': min_prob,
    'staircase_seq_len_source': staircase_seq_len_source,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'fast_bw',
    'from': source,
    **args}, name=name or 'fast_baum_welch')


# noinspection PyShadowingBuiltins,PyShadowingNames
def synthetic_gradient(
                       source: LayerRef,
                       *,
                       gradient: LayerRef,
                       meta_loss_scale: float = NotSpecified,
                       name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is a generalized way to be able to replace the true gradient with any kind of predicted gradient.
  This enabled to implement the idea from here:
    Decoupled Neural Interfaces using Synthetic Gradients, https://arxiv.org/abs/1608.05343

  :param LayerRef source:
  :param LayerBase gradient:
  :param float meta_loss_scale:
  :param str|NameCtx|None name:
  """
  args = {
    'gradient': gradient,
    'meta_loss_scale': meta_loss_scale,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'synthetic_gradient',
    'from': source,
    **args}, name=name or 'synthetic_gradient')


# noinspection PyShadowingBuiltins,PyShadowingNames
def tikhonov_regularization(
                            source: LayerRef,
                            *,
                            meta_loss_scale: float = NotSpecified,
                            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds the Tikhonov regularization as a meta-loss (see :class:`returnn.tf.util.basic.MetaLosses`).

  :param LayerRef source:
  :param float meta_loss_scale:
  :param str|NameCtx|None name:
  """
  args = {
    'meta_loss_scale': meta_loss_scale,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'tikhonov_regularization',
    'from': source,
    **args}, name=name or 'tikhonov_regularization')


# noinspection PyShadowingBuiltins,PyShadowingNames
def print(
          source: LayerRef,
          *,
          summarize: Optional[int] = NotSpecified,
          extra_print_args: Union[List, Tuple] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Prints the sources to console/log, via :func:`returnn.tf.util.basic.py_print`.

  :param LayerRef source:
  :param int|None summarize: passed to :func:`py_print`
  :param list|tuple extra_print_args:
  :param str|NameCtx|None name:
  """
  args = {
    'summarize': summarize,
    'extra_print_args': extra_print_args,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'print',
    'from': source,
    **args}, name=name or 'print')


# noinspection PyShadowingBuiltins,PyShadowingNames
def hdf_dump(
             source: LayerRef,
             *,
             filename: Union[str, callable],
             extra: Optional[Dict[str, LayerRef]] = NotSpecified,
             dump_whole_batches: bool = NotSpecified,
             labels: Optional[List[str]] = NotSpecified,
             extend_existing_file: bool = NotSpecified,
             dump_per_run: bool = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Dumps into HDF file, compatible to :class:`HDFDataset`.

  The HDF will be written to disk under the specified filename, if there was no error,
  by default at graph reset, via :func:`TFNetwork.register_graph_reset_callback`.
  Or after the dataset iteration run loop, with dump_per_run,
  via :func:`TFNetwork.register_run_finished_callback`.

  Common usage would be to add this to your network with "is_output_layer": True,
  such that you don't need to make other layers depend on it.

  It currently uses :class:`SimpleHDFWriter` internally.

  :param LayerRef source:
  :param str|(()->str) filename:
  :param None|dict[str,LayerBase] extra:
  :param bool dump_whole_batches: dumps the whole batch as a single sequence into the HDF
  :param list[str]|None labels:
  :param bool extend_existing_file: True also means we expect that it exists
  :param bool dump_per_run: write via :func:`TFNetwork.register_run_finished_callback`
  :param str|NameCtx|None name:
  """
  args = {
    'filename': filename,
    'extra': extra,
    'dump_whole_batches': dump_whole_batches,
    'labels': labels,
    'extend_existing_file': extend_existing_file,
    'dump_per_run': dump_per_run,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'hdf_dump',
    'from': source,
    **args}, name=name or 'hdf_dump')


class _Rec(_Base):
  """
  Recurrent layer, has support for several implementations of LSTMs (via ``unit`` argument),
  see :ref:`tf_lstm_benchmark` (https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html),
  and also GRU, or simple RNN.
  Via `unit` parameter, you specify the operation/model performed in the recurrence.
  It can be a string and specify a RNN cell, where all TF cells can be used,
  and the `"Cell"` suffix can be omitted; and case is ignored.
  Some possible LSTM implementations are (in all cases for both CPU and GPU):

   * BasicLSTM (the cell), via official TF, pure TF implementation
   * LSTMBlock (the cell), via tf.contrib.rnn.
   * LSTMBlockFused, via tf.contrib.rnn. should be much faster than BasicLSTM
   * CudnnLSTM, via tf.contrib.cudnn_rnn. This is experimental yet.
   * NativeLSTM, our own native LSTM. should be faster than LSTMBlockFused.
   * NativeLstm2, improved own native LSTM, should be the fastest and most powerful.

  We default to the current tested fastest one, i.e. NativeLSTM.
  Note that they are currently not compatible to each other, i.e. the way the parameters are represented.

  A subnetwork can also be given which will be evaluated step-by-step,
  which can use attention over some separate input,
  which can be used to implement a decoder in a sequence-to-sequence scenario.
  The subnetwork will get the extern data from the parent net as templates,
  and if there is input to the RecLayer,
  then it will be available as the "source" data key in the subnetwork.
  The subnetwork is specified as a `dict` for the `unit` parameter.
  In the subnetwork, you can access outputs from layers from the previous time step when they
  are referred to with the "prev:" prefix.

  Example::

      {
          "class": "rec",
          "from": "input",
          "unit": {
            # Recurrent subnet here, operate on a single time-step:
            "output": {
              "class": "linear",
              "from": ["prev:output", "data:source"],
              "activation": "relu",
              "n_out": n_out},
          },
          "n_out": n_out},
      }

  More examples can be seen in :mod:`test_TFNetworkRecLayer` and :mod:`test_TFEngine`.

  The subnetwork can automatically optimize the inner recurrent loop
  by moving layers out of the loop if possible.
  It will try to do that greedily. This can be disabled via the option `optimize_move_layers_out`.
  It assumes that those layers behave the same with time-dimension or without time-dimension and used per-step.
  Examples for such layers are :class:`LinearLayer`, :class:`RnnCellLayer`
  or :class:`SelfAttentionLayer` with option `attention_left_only`.

  This layer can also be inside another RecLayer. In that case, it behaves similar to :class:`RnnCellLayer`.
  (This support is somewhat incomplete yet. It should work for the native units such as NativeLstm.)

  Also see :ref:`recurrency`.
  """
  returnn_layer_class = 'rec'
  has_recurrent_state = True
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               out_dim: Dim,
               *,
               unit: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               direction: Optional[int] = NotSpecified,
               input_projection: bool = NotSpecified,
               max_seq_len: Optional[Union[str, int]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               optimize_move_layers_out: Optional[bool] = NotSpecified,
               cheating: bool = NotSpecified,
               unroll: bool = NotSpecified,
               back_prop: Optional[bool] = NotSpecified,
               use_global_rec_step_offset: bool = NotSpecified,
               include_eos: bool = NotSpecified,
               debug: Optional[bool] = NotSpecified,
               **kwargs):
    """
    :param Dim out_dim: output feature dimension
    :param str|_SubnetworkRecCell unit: the RNNCell/etc name, e.g. "nativelstm". see comment below.
      alternatively a whole subnetwork, which will be executed step by step,
      and which can include "prev" in addition to "from" to refer to previous steps.
      The subnetwork is specified as a net dict in the config.
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param int|None direction: None|1 -> forward, -1 -> backward
    :param bool input_projection: True -> input is multiplied with matrix. False only works if same input dim
    :param int|tf.Tensor|None max_seq_len: if unit is a subnetwork. str will be evaluated. see code
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str recurrent_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param bool|None optimize_move_layers_out: will automatically move layers out of the loop when possible
    :param bool cheating: Unused, is now part of ChoiceLayer
    :param bool unroll: if possible, unroll the loop (implementation detail)
    :param bool|None back_prop: for tf.while_loop. the default will use self.network.train_flag
    :param bool use_global_rec_step_offset:
    :param bool include_eos: for search, whether we should include the frame where "end" is True
    :param bool|None debug:
    """
    super().__init__(**kwargs)
    self.out_dim = out_dim
    self.unit = unit
    self.unit_opts = unit_opts
    self.direction = direction
    self.input_projection = input_projection
    self.max_seq_len = max_seq_len
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init
    self.optimize_move_layers_out = optimize_move_layers_out
    self.cheating = cheating
    self.unroll = unroll
    self.back_prop = back_prop
    self.use_global_rec_step_offset = use_global_rec_step_offset
    self.include_eos = include_eos
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'out_dim': self.out_dim,
      'unit': self.unit,
      'unit_opts': self.unit_opts,
      'direction': self.direction,
      'input_projection': self.input_projection,
      'max_seq_len': self.max_seq_len,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
      'optimize_move_layers_out': self.optimize_move_layers_out,
      'cheating': self.cheating,
      'unroll': self.unroll,
      'back_prop': self.back_prop,
      'use_global_rec_step_offset': self.use_global_rec_step_offset,
      'include_eos': self.include_eos,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef = (),
               *,
               state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               axis: Dim,
               ) -> Tuple[Layer, LayerState]:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    self.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
    layer = make_layer({
      'class': 'rec',
      'from': source,
      **args,
      **self.get_opts()}, module=self)
    out_state = self.returnn_layer_get_recurrent_state(layer)
    return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def _get_last_hidden_state(
                           source: LayerRef,
                           *,
                           out_dim: Dim,
                           combine: str = NotSpecified,
                           key: Optional[Union[str, int]] = NotSpecified,
                           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Will combine (concat or add or so) all the last hidden states from all sources.

  :param LayerRef source:
  :param Dim out_dim: output feature dimension
  :param str combine: "concat" or "add"
  :param str|int|None key: for the state, which could be a namedtuple. see :func:`RnnCellLayer.get_state_by_key`
  :param str|NameCtx|None name:
  """
  args = {
    'out_dim': out_dim,
    'combine': combine,
    'key': key,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'get_last_hidden_state',
    'from': source,
    **args}, name=name or 'get_last_hidden_state')


# noinspection PyShadowingBuiltins,PyShadowingNames
def rec_unstack(
                source: LayerRef,
                *,
                axis: Dim,
                declare_rec_time: bool = NotSpecified,
                name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is supposed to be used inside a :class:`RecLayer`.
  The input is supposed to be outside the rec layer (i.e. via ``base:``).
  Uses tf.TensorArray and then unstack on the inputs to make it available per-frame.
  This is an alternative to making some input to the rec layer,
  such that the rec layer can have multiple inputs (as long as they have the same time dim).

  Note that due to automatic optimization, this layer will be optimized out of the rec loop anyway,
  and then the tf.TensorArray logic happens internally in RecLayer,
  thus we do not need to care about this here.
  (See get_input_moved_out for some internal handling.)

  Effectively, this layer is very similar to :class:`CopyLayer`,
  with the only special behavior that it checks (or even assigns) the loop dimension of RecLayer.

  Due to automatic optimization, not much happens here.
  The real logic happens in :func:`get_out_data_from_opts`.

  Note that it is allowed to leave both `axis` and `declare_rec_time` unset,
  in case you assign `axis` to the rec layer, and the source here has the same axis (dim tag).

  :param LayerRef source:
  :param Dim axis:
  :param bool declare_rec_time:
  :param str|NameCtx|None name:
  """
  args = {
    'axis': axis,
    'declare_rec_time': declare_rec_time,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'rec_unstack',
    'from': source,
    **args}, name=name or 'rec_unstack')


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice(
           source: LayerRef,
           *,
           target: LayerRef,
           beam_size: int,
           keep_beams: bool = NotSpecified,
           search: Union[NotSpecified, bool] = NotSpecified,
           input_type: str = NotSpecified,
           prob_scale: float = NotSpecified,
           base_beam_score_scale: float = NotSpecified,
           random_sample_scale: float = NotSpecified,
           length_normalization: bool = NotSpecified,
           length_normalization_exponent: Any = NotSpecified,
           custom_score_combine: Optional[callable] = NotSpecified,
           source_beam_sizes: Optional[List[int]] = NotSpecified,
           scheduled_sampling: Optional[Dict] = NotSpecified,
           cheating: Union[bool, str] = NotSpecified,
           explicit_search_sources: Optional[List[LayerRef]] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This layer represents a choice to be made in search during inference,
  such as choosing the top-k outputs from a log-softmax for beam search.
  During training, this layer can return the true label.
  This is supposed to be used inside the rec layer.
  This can be extended in various ways.

  We present the scores in +log space, and we will add them up along the path.
  Assume that we get input (batch,dim) from a (log-)softmax.
  Assume that each batch is already a choice via search.
  In search with a beam size of N, we would output
  sparse (batch=N,) and scores for each.

  In case of multiple sources, this layer computes the top-k combinations of choices. The score of such a combination
  is determined by adding up the (log-space) scores of the choices for the individual sources. In this case, the
  'target' parameter of the layer has to be set to a list of targets corresponding to the sources respectively. Because
  computing all possible combinations of source scores is costly, the sources are pruned beforehand using the beam
  sizes set by the 'source_beam_sizes' parameter. The choices made for the different sources can be accessed via the
  sublayers '<choice layer name>/out_0', '<choice layer name>/out_1' and so on.
  Note, that the way scores are combined assumes the sources to be independent. If you want to model a dependency,
  use separate ChoiceLayers and let the input of one depend on the output of the other.

  :param LayerRef source:
  :param LayerBase target: target
  :param int beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
  :param bool keep_beams: specifies that we keep the beam_in entries,
    i.e. we just expand, i.e. we just search on the dim. beam_size must be a multiple of beam_in.
  :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
    If not specified, it will depend on `network.search_flag`.
  :param str input_type: "prob" or "log_prob", whether the input is in probability space, log-space, etc.
    or "regression", if it is a prediction of the data as-is. If there are several inputs, same format
    for all is assumed.
  :param float prob_scale: factor for prob (score in +log space from source)
  :param float base_beam_score_scale: factor for beam base score (i.e. prev prob scores)
  :param float random_sample_scale: if >0, will add Gumbel scores. you might want to set base_beam_score_scale=0
  :param bool length_normalization: evaluates score_t/len in search
  :param length_normalization_exponent:
  :param callable|None custom_score_combine:
  :param list[int]|None source_beam_sizes: If there are several sources, they are pruned with these beam sizes
     before combination. If None, 'beam_size' is used for all sources. Has to have same length as number of sources.
  :param dict|None scheduled_sampling:
  :param bool|str cheating: if True, will always add the true target in the beam.
    if "exclusive", enables cheating_exclusive. see :func:`returnn.tf.util.basic.beam_search`.
  :param list[LayerBase]|None explicit_search_sources: will mark it as an additional dependency.
    You might use these also in custom_score_combine.
  :param str|NameCtx|None name:
  """
  args = {
    'target': target,
    'beam_size': beam_size,
    'keep_beams': keep_beams,
    'search': search,
    'input_type': input_type,
    'prob_scale': prob_scale,
    'base_beam_score_scale': base_beam_score_scale,
    'random_sample_scale': random_sample_scale,
    'length_normalization': length_normalization,
    'length_normalization_exponent': length_normalization_exponent,
    'custom_score_combine': custom_score_combine,
    'source_beam_sizes': source_beam_sizes,
    'scheduled_sampling': scheduled_sampling,
    'cheating': cheating,
    'explicit_search_sources': explicit_search_sources,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'choice',
    'from': source,
    **args}, name=name or 'choice')


# noinspection PyShadowingBuiltins,PyShadowingNames
def decide(
           source: LayerRef,
           *,
           length_normalization: bool = NotSpecified,
           search: Union[NotSpecified, bool] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is kind of the counter-part to the choice layer.
  This only has an effect in search mode.
  E.g. assume that the input is of shape (batch * beam, time, dim)
  and has search_sources set.
  Then this will output (batch, time, dim) where the beam with the highest score is selected.
  Thus, this will do a decision based on the scores.
  In will convert the data to batch-major mode.

  :param LayerRef source:
  :param bool length_normalization: performed on the beam scores
  :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
    If not specified, it will depend on `network.search_flag`.
  :param str|NameCtx|None name:
  """
  args = {
    'length_normalization': length_normalization,
    'search': search,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'decide',
    'from': source,
    **args}, name=name or 'decide')


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice_get_beam_scores(
                           source: LayerRef,
                           *,
                           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gets beam scores from :class:`SearchChoices`.
  This requires that the source has search choices.

  .. note::

    This layer might be deprecated in the future.


  :param LayerRef source:
  :param str|NameCtx|None name:
  """
  return make_layer({
    'class': 'choice_get_beam_scores',
    'from': source,
    }, name=name or 'choice_get_beam_scores')


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice_get_src_beams(
                         source: LayerRef,
                         *,
                         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gets source beam indices from :class:`SearchChoices`.
  This requires that the source has search choices.

  :param LayerRef source:
  :param str|NameCtx|None name:
  """
  return make_layer({
    'class': 'choice_get_src_beams',
    'from': source,
    }, name=name or 'choice_get_src_beams')


# noinspection PyShadowingBuiltins,PyShadowingNames
def split_batch_beam(
                     source: LayerRef,
                     *,
                     beam_dim: Optional[Dim] = NotSpecified,
                     search: Union[NotSpecified, bool] = NotSpecified,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Splits the batch dimension of the input, which includes a beam, into (batch,beam).

  Like :class:`DecideLayer`, this removes the beam.

  :param LayerRef source:
  :param Dim|None beam_dim:
  :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
    If not specified, it will depend on `network.search_flag`.
  :param str|NameCtx|None name:
  """
  args = {
    'beam_dim': beam_dim,
    'search': search,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'split_batch_beam',
    'from': source,
    **args}, name=name or 'split_batch_beam')


# noinspection PyShadowingBuiltins,PyShadowingNames
def positional_encoding(
                        source: LayerRef,
                        *,
                        out_dim: Dim,
                        add_to_input: bool = NotSpecified,
                        constant: int = NotSpecified,
                        offset: Optional[LayerRef] = NotSpecified,
                        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Provides positional encoding in the form of (batch, time, n_out) or (time, batch, n_out)
  where n_out is the number of channels, if it is run outside a :class:`RecLayer`,
  and (batch, n_out) or (n_out, batch)
  if run inside a :class:`RecLayer`, where it will depend on the current time frame.

  Assumes one source input with a time dimension if outside a :class:`RecLayer`.
  With `add_to_input`, it will calculate `x + input`, and the output shape is the same as the input

  The positional encoding is the same as in Tensor2Tensor.
  See :func:`returnn.tf.util.basic.get_positional_encoding`.

  :param LayerRef source:
  :param Dim out_dim: output feature dimension
  :param bool add_to_input: will add the signal to the input
  :param int constant: if positive, always output the corresponding positional encoding.
  :param None|LayerBase offset: Specify the offset to be added to positions. Expect shape (batch, time) or (batch,).
  :param str|NameCtx|None name:
  """
  args = {
    'out_dim': out_dim,
    'add_to_input': add_to_input,
    'constant': constant,
    'offset': offset,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'positional_encoding',
    'from': source,
    **args}, name=name or 'positional_encoding')


# noinspection PyShadowingBuiltins,PyShadowingNames
def ken_lm_state(
                 source: LayerRef,
                 *,
                 state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
                 initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
                 lm_file: callable,
                 vocab_file: Optional[str] = NotSpecified,
                 vocab_unknown_label: str = NotSpecified,
                 bpe_merge_symbol: Optional[str] = NotSpecified,
                 input_step_offset: int = NotSpecified,
                 dense_output: bool = NotSpecified,
                 debug: bool = NotSpecified,
                 axis: Dim,
                 name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  Get next word (or subword) each frame,
  accumulates string,
  keeps state of seen string so far,
  returns score (+log space, natural base e) of sequence,
  using KenLM (https://kheafield.com/code/kenlm/) (see :mod:`TFKenLM`).
  EOS (</s>) token must be used explicitly.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param str|()->str lm_file: ARPA file or so. whatever KenLM supports
  :param str|None vocab_file: if the inputs are symbols, this must be provided. see :class:`Vocabulary`
  :param str vocab_unknown_label: for the vocabulary
  :param str|None bpe_merge_symbol: e.g. "@@" if you want to apply BPE merging
  :param int input_step_offset: if provided, will consider the input only from this step onwards
  :param bool dense_output: whether we output the score for all possible succeeding tokens
  :param bool debug: prints debug info
  :param Dim axis: axis to operate over, or nn.single_step_dim
  :param str|NameCtx|None name:
  """
  args = {
    'lm_file': lm_file,
    'vocab_file': vocab_file,
    'vocab_unknown_label': vocab_unknown_label,
    'bpe_merge_symbol': bpe_merge_symbol,
    'input_step_offset': input_step_offset,
    'dense_output': dense_output,
    'debug': debug,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'kenlm',
    'from': source,
    **args}, name=name or 'ken_lm_state')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def edit_distance_table(
                        source: LayerRef,
                        *,
                        state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
                        initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
                        debug: bool = NotSpecified,
                        blank_idx: Optional[int] = NotSpecified,
                        out_dim: Optional[Dim] = NotSpecified,
                        axis: Dim,
                        name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  Given a source and a target, calculates the edit distance table between them.
  Source can be inside a recurrent loop.
  It uses :func:`TFNativeOp.next_edit_distance_row`.

  Usually, if you are inside a rec layer, and "output" is the :class:`ChoiceLayer`,
  you would use "from": "output"
  and "target": "layer:base:data:target" (make sure it has the time dimension).

  See also :class:`OptimalCompletionsLayer`.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param bool debug:
  :param int|None blank_idx: if given, will keep the same row for this source label
  :param Dim|None out_dim:
  :param Dim axis: axis to operate over, or nn.single_step_dim
  :param str|NameCtx|None name:
  """
  args = {
    'debug': debug,
    'blank_idx': blank_idx,
    'out_dim': out_dim,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'edit_distance_table',
    'from': source,
    **args}, name=name or 'edit_distance_table')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def optimal_completions(
                        source: LayerRef,
                        *,
                        debug: bool = NotSpecified,
                        blank_idx: Optional[int] = NotSpecified,
                        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  We expect to get the inputs from :class:`EditDistanceTableLayer`, esp from the prev frame, like this:
  "opt_completions": {"class": "optimal_completions", "from": "prev:edit_dist_table"}.

  You can also then define this further layer:
  "opt_completion_soft_targets": {
    "class": "eval", "eval": "tf.nn.softmax(tf.cast(source(0), tf.float32))",
    "from": "opt_completions", "out_type": {"dtype": "float32"}},
  and use that as the :class:`CrossEntropyLoss` soft targets
  for the input of the "output" :class:`ChoiceLayer`, e.g. "output_prob".
  This makes most sense when you enable beam search (even, or esp, during training).
  Note that you probably want to have this all before the last choice, where you still have more beams open.

  :param LayerRef source:
  :param bool debug:
  :param int|None blank_idx:
  :param str|NameCtx|None name:
  """
  args = {
    'debug': debug,
    'blank_idx': blank_idx,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  return make_layer({
    'class': 'optimal_completions',
    'from': source,
    **args}, name=name or 'optimal_completions')


# noinspection PyShadowingBuiltins,PyShadowingNames
def unmask(
           source: LayerRef,
           *,
           state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
           mask: LayerRef,
           axis: Dim,
           name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  This is meant to be used together with :class:`MaskedComputationLayer`,
  which operates on input [B,T,D], and given a mask, returns [B,T',D'].
  This layer :class:`UnmaskLayer` is supposed to undo the masking,
  i.e. to recover the original time dimension, i.e. given [B,T',D'], we output [B,T,D'].
  This is done by repeating the output for the non-masked frames,
  via the last masked frame.

  If this layer is inside a recurrent loop, i.e. we get [B,D'] as input,
  this is a no-op, and we just return the input as is.
  In that case, the repetition logic is handled via :class:`MaskedComputationLayer`.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param LayerBase mask: the same as as used for :class:`MaskedComputationLayer`.
    Outside loop: [B,T] or [T,B], original T. Inside loop, just [B].
  :param Dim axis: axis to operate over, or nn.single_step_dim
  :param str|NameCtx|None name:
  """
  args = {
    'mask': mask,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'unmask',
    'from': source,
    **args}, name=name or 'unmask')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state


class _TwoDLSTM(_Base):
  """
  2D LSTM.

  Currently only from left-to-right in the time axis.
  Can be inside a recurrent loop, or outside.
  """
  returnn_layer_class = 'twod_lstm'
  has_recurrent_state = True
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               pooling: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               **kwargs):
    """
    :param str pooling: defines how the 1D return value is computed based on the 2D lstm result. Either 'last' or 'max'
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str recurrent_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    """
    super().__init__(**kwargs)
    self.pooling = pooling
    self.unit_opts = unit_opts
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'pooling': self.pooling,
      'unit_opts': self.unit_opts,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __call__(self,
               source: LayerRef,
               *,
               state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               axis: Dim,
               ) -> Tuple[Layer, LayerState]:
    """
    Make layer dict
    """
    assert isinstance(source, LayerRef)
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    self.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
    layer = make_layer({
      'class': 'twod_lstm',
      'from': source,
      **args,
      **self.get_opts()}, module=self)
    out_state = self.returnn_layer_get_recurrent_state(layer)
    return layer, out_state


# noinspection PyShadowingBuiltins,PyShadowingNames
def cum_concat(
               source: LayerRef,
               *,
               state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               initial_state: Optional[Union[LayerRef, Dict[str, LayerRef], NotSpecified]] = NotSpecified,
               out_spatial_dim: Dim,
               axis: Dim,
               name: Optional[Union[str, NameCtx]] = None) -> Tuple[Layer, LayerState]:
  """
  Concatenates all previous frames of a time-axis.
  Like :class:`CumsumLayer` uses `sum`, this layer uses `concat`.

  This layer can be used as a base for auto-regressive self-attention.

  This layer expects to be inside a :class:`RecLayer`.

  Inside a rec loop (not optimized out),
  this will concatenate the current input
  to the previous accumulated inputs.
  For an input of shape `input_shape`,
  it will output a tensor of shape `[new_dim] + input_shape`.
  `new_dim` (``out_spatial_dim``) is a special dimension, usually of length `i`,
  where `i` is the current loop frame,
  i.e. the length increases in every loop frame.
  `new_dim` is specified by a separate own dim tag.
  For example, in the first frame,
  this will be of shape `[1] + input_shape`,
  in the second frame shape `[2] + input_shape`,
  and so on,
  and in the last frame shape `[T] + input_shape`.

  Outside the rec loop (optimized out),
  this layer expects an input with the time dim of the rec layer,
  and returns the input as-is,
  but replacing the time dim tag with the dim tag `new_dim`
  converted as outside the loop.

  Normally the optimization should not matter for the user,
  i.e. for the user, the logical behavior is always as being inside the rec loop.
  Outside the loop,
  the output represents a tensor of shape `[T, new_dim] + input_shape`,
  although we actually have another `new_dim` outside the loop,
  and `T` is not actually there,
  but we still have all the information,
  because the last frame has all information.
  This `new_dim` outside the loop stores all the dynamic seq lengths
  per frame of the loop, i.e. the dyn seq len are extended of shape [B,T] or [T]
  (unlike usually just [B]).
  This way following layers use different seq lengths of `new_dim` for different loop frames,
  just like if the `T` dim would actually exist.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None initial_state:
  :param Dim out_spatial_dim:
  :param Dim axis: axis to operate over, or nn.single_step_dim
  :param str|NameCtx|None name:
  """
  args = {
    'out_spatial_dim': out_spatial_dim,
    'axis': axis,
    }
  args = {key: value for (key, value) in args.items() if value is not NotSpecified}
  _ReturnnWrappedLayerBase.handle_recurrent_state(args, axis=axis, state=state, initial_state=initial_state)
  layer = make_layer({
    'class': 'cum_concat',
    'from': source,
    **args}, name=name or 'cum_concat')
  out_state = _ReturnnWrappedLayerBase.returnn_layer_get_recurrent_state(layer)
  return layer, out_state
